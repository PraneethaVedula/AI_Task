# -*- coding: utf-8 -*-
"""Peritys,InteraxAI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u-nXZA74GH4oa5rKgYayICdiJa4QLdvG

Praneetha Vedula
"""

!pip install torch torchaudio numpy librosa matplotlib pandas

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/My\ Drive/

data_dir = '/content/drive/My Drive/svarah/svarah/audio'  # This is the folder containing both audio and text files
metadata_path = '/content/drive/My Drive/svarah/svarah/meta_speaker_stats.csv'  # Path to metadata

import os
import pandas as pd
import librosa
import matplotlib.pyplot as plt

# Define the directory containing your audio and text files
data_dir = '/content/drive/My Drive/svarah/svarah/audio'  # Adjust this if needed based on your structure

# Initialize lists to hold audio data and corresponding texts
audio_data = []
texts = []

# List all files in the directory
for filename in os.listdir(data_dir):
    if filename.endswith('.wav'):  # Assuming your audio files are .wav format
        # Load the audio file
        audio_file_path = os.path.join(data_dir, filename)
        y, sr = librosa.load(audio_file_path, sr=None)

        # Append the audio data to the list
        audio_data.append((filename, y, sr))

        # Load the corresponding text file
        text_file_name = os.path.splitext(filename)[0] + '.txt'  # Change extension to .txt
        text_file_path = os.path.join(data_dir, text_file_name)

        # Read the text file if it exists
        if os.path.exists(text_file_path):
            with open(text_file_path, 'r', encoding='utf-8') as f:
                text = f.read()
                texts.append((filename, text))  # Store the filename and corresponding text
        else:
            print(f"Text file not found for {filename}")

# Optionally display some loaded data
for i in range(min(1, len(audio_data))):  # Display first 5 samples
    print(f"Audio File: {audio_data[i][0]}, Length: {len(audio_data[i][1])} samples, Sample Rate: {audio_data[i][2]}")
    print(f"Text: {texts[i][1][:100]}...")  # Print the first 100 characters of the text

import librosa
import numpy as np

def mel_spectrogram(y, sr):
    # Compute Mel spectrogram
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)
    # Convert to decibels
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    return mel_spec_db

mel_specs = []
for audio_info in audio_data:
    mel_spec = mel_spectrogram(audio_info[1], audio_info[2])
    mel_specs.append((audio_info[0], mel_spec))

# Display the shape of the Mel spectrogram for the first audio file
if mel_specs:
    print(f'Mel Spectrogram Shape for {mel_specs[0][0]}: {mel_specs[0][1].shape}')

# Creating a dataset structure
dataset = []
for (audio_info, text_info) in zip(audio_data, texts):
    dataset.append((audio_info[1], mel_spectrogram(audio_info[1], audio_info[2]), text_info[1]))

from sklearn.model_selection import train_test_split

# Split the dataset into training and testing datasets
train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)
val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)

print(f"Training samples: {len(train_data)}")
print(f"Validation samples: {len(val_data)}")
print(f"Test samples: {len(test_data)}")

import torch
from torch.utils.data import Dataset, DataLoader

class AudioTextDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        mel_spec = self.dataset[idx][1]  # Mel spectrogram
        text = self.dataset[idx][2]       # Corresponding text
        return torch.tensor(mel_spec), text  # Converting to tensor

train_dataset = AudioTextDataset(train_data)
val_dataset = AudioTextDataset(val_data)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/jaywalnut310/vits  # Adjust accordingly
# %cd vits

import sys
print(sys.version)

!pip install --upgrade pip setuptools wheel

!apt-get install build-essential

!pip install virtualenv
!virtualenv myenv
# or for Windows
!myenv\Scripts\activate

pip install -r requirements.txt

pip install numpy

# Create the config directory and YAML file
import os

# Create the configs directory
os.makedirs('configs', exist_ok=True)

# Create the YAML configuration file
config_content = """
batch_size: 32
learning_rate: 0.001
epochs: 100
model_dir: './model'
"""

with open('configs/your_config_file.yaml', 'w') as f:
    f.write(config_content)

import yaml

# Load configuration
with open('configs/your_config_file.yaml', 'r') as file:
    config = yaml.safe_load(file)

# Access the parameters
batch_size = config['batch_size']
learning_rate = config['learning_rate']
epochs = config['epochs']
model_dir = config['model_dir']

# Example of defining a simple model (modify this part as you need)
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Sequential(nn.Linear(10, 1))  # Example model, change to your model
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Dummy training loop
for epoch in range(epochs):
    # Replace with your actual training code
    print(f'Epoch {epoch+1}/{epochs}')

# Save the model
torch.save(model.state_dict(), 'model_weights.pth')

!pip install numpy==1.19.5

# Install only the essential requirements from VITS, skipping version conflicts
!pip install Cython==0.29.21 --no-deps
!pip install phonemizer --no-deps

import os
os.makedirs('/content/vits/vits/monotonic_align', exist_ok=True)

# Commented out IPython magic to ensure Python compatibility.
# Navigate to the monotonic_align directory and compile it
# %cd monotonic_align
!python setup.py build_ext --inplace
# %cd ..

# Commented out IPython magic to ensure Python compatibility.
# Install necessary packages
!pip install torch torchaudio numpy librosa matplotlib pandas

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define data directory and metadata path
data_dir = '/content/drive/My Drive/svarah/svarah/audio'
metadata_path = '/content/drive/My Drive/svarah/svarah/meta_speaker_stats.csv'

import os
import pandas as pd
import librosa
import matplotlib.pyplot as plt

# Initialize lists to hold audio data and corresponding texts
audio_data = []
texts = []

# Load audio and corresponding text files
for filename in os.listdir(data_dir):
    if filename.endswith('.wav'):
        audio_file_path = os.path.join(data_dir, filename)
        y, sr = librosa.load(audio_file_path, sr=None)
        audio_data.append((filename, y, sr))

        text_file_name = os.path.splitext(filename)[0] + '.txt'
        text_file_path = os.path.join(data_dir, text_file_name)

        if os.path.exists(text_file_path):
            with open(text_file_path, 'r', encoding='utf-8') as f:
                text = f.read()
                texts.append((filename, text))
        else:
            print(f"Text file not found for {filename}")

# Function to create mel spectrogram
def mel_spectrogram(y, sr):
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80, fmax=8000)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    return mel_spec_db

# Prepare mel spectrograms
mel_specs = []
for audio_info in audio_data:
    mel_spec = mel_spectrogram(audio_info[1], audio_info[2])
    mel_specs.append((audio_info[0], mel_spec))

# Creating a dataset structure
dataset = []
for (audio_info, text_info) in zip(audio_data, texts):
    dataset.append((audio_info[1], mel_spectrogram(audio_info[1], audio_info[2]), text_info[1]))

# Split the dataset into training and testing datasets
from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)
val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)

# Create a custom dataset
import torch
from torch.utils.data import Dataset, DataLoader

class AudioTextDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        mel_spec = self.dataset[idx][1]  # Mel spectrogram
        text = self.dataset[idx][2]       # Corresponding text
        return torch.tensor(mel_spec, dtype=torch.float32), text  # Ensure dtype is correct

train_dataset = AudioTextDataset(train_data)
val_dataset = AudioTextDataset(val_data)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Clone VITS repository
!git clone https://github.com/jaywalnut310/vits
# %cd vits

# Install dependencies
!pip install -r requirements.txt

# Create the config directory and YAML file
import os
os.makedirs('configs', exist_ok=True)

# Create the YAML configuration file
config_content = """
batch_size: 32
learning_rate: 0.001
epochs: 100
model_dir: './model'
"""

with open('configs/your_config_file.yaml', 'w') as f:
    f.write(config_content)

# Load configuration
import yaml
with open('configs/your_config_file.yaml', 'r') as file:
    config = yaml.safe_load(file)

# Access the parameters
batch_size = config['batch_size']
learning_rate = config['learning_rate']
epochs = config['epochs']
model_dir = config['model_dir']

# Assuming necessary packages are installed, attempt the model import
import sys
sys.path.insert(0, './')  # Add VITS to the path

try:
    from models import VITS  # Import the actual VITS model
    print("VITS model imported successfully!")
except ModuleNotFoundError as e:
    print(f"Error importing VITS: {e}")

# # Import the VITS model from the cloned repository
# import sys
# sys.path.insert(0, './')  # Add VITS to the path
# from models import VITS  # Import the actual VITS model

# # Initialize the model
# model = VITS()  # Make sure to adjust the model initialization if needed

# Set up the optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Define a loss function (adjust based on your use case)
def loss_function(outputs, labels):
    mse_loss = torch.nn.MSELoss()(outputs, labels)
    # Add other losses as necessary
    return mse_loss

# Training loop
for epoch in range(epochs):
    model.train()  # Set the model to training mode
    for mel_spectrogram, text in train_loader:
        optimizer.zero_grad()

        # Forward pass
        mel_spectrogram = mel_spectrogram.unsqueeze(1)  # Adjust shape if necessary
        outputs = model(mel_spectrogram)  # Adjust as per your model's forward function

        # Calculate loss
        labels = ...  # Encode text to numerical labels if necessary
        loss = loss_function(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')  # Print the loss at the end of each epoch

# Save the model
torch.save(model.state_dict(), 'model_weights.pth')

!pip install --upgrade numpy

# Commented out IPython magic to ensure Python compatibility.
# Install necessary packages
!pip install torch torchaudio librosa matplotlib pandas

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define data directory and metadata path
data_dir = '/content/drive/My Drive/svarah/svarah/audio' # https://drive.google.com/drive/folders/1kG7y-oq7ZZBwghT5cQpnj3Pd5SKbKuv4?usp=drive_link
metadata_path = '/content/drive/My Drive/svarah/svarah/meta_speaker_stats.csv'

import os
import pandas as pd
import librosa
import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to hold audio data and corresponding texts
audio_data = []
texts = []

# Load audio and corresponding text files
for filename in os.listdir(data_dir):
    if filename.endswith('.wav'):
        audio_file_path = os.path.join(data_dir, filename)
        # y, sr = librosa.load(audio_file_path, sr=None)
        y, sr = librosa.load(audio_file_path, sr=None, duration=5)  # Loads only the first 5 seconds

        audio_data.append((filename, y, sr))

        text_file_name = os.path.splitext(filename)[0] + '.txt'
        text_file_path = os.path.join(data_dir, text_file_name)

        if os.path.exists(text_file_path):
            with open(text_file_path, 'r', encoding='utf-8') as f:
                text = f.read()
                texts.append((filename, text))
        else:
            print(f"Text file not found for {filename}")

# Function to create mel spectrogram
def mel_spectrogram(y, sr):
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80, fmax=8000)
    mel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)
    return mel_spec_db

# Prepare mel spectrograms
mel_specs = []
for audio_info in audio_data:
    mel_spec = mel_spectrogram(audio_info[1], audio_info[2])
    mel_specs.append((audio_info[0], mel_spec))

# Creating a dataset structure
dataset = []
for (audio_info, text_info) in zip(audio_data, texts):
    dataset.append((audio_info[1], mel_spectrogram(audio_info[1], audio_info[2]), text_info[1]))

# Split the dataset into training and testing datasets
from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)
val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)

# Create a custom dataset
import torch
from torch.utils.data import Dataset, DataLoader

class AudioTextDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        mel_spec = self.dataset[idx][1]  # Mel spectrogram
        text = self.dataset[idx][2]       # Corresponding text
        return torch.tensor(mel_spec, dtype=torch.float32), text  # Ensure dtype is correct

train_dataset = AudioTextDataset(train_data)
val_dataset = AudioTextDataset(val_data)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Clone VITS repository
!git clone https://github.com/jaywalnut310/vits
# %cd vits

# Install dependencies
!pip install -r requirements.txt

# Create the config directory and YAML file
import os
os.makedirs('configs', exist_ok=True)

# Create the YAML configuration file
config_content = """
batch_size: 32
learning_rate: 0.001
epochs: 100
model_dir: './model'
"""

with open('configs/your_config_file.yaml', 'w') as f:
    f.write(config_content)

# Load configuration
import yaml
with open('configs/your_config_file.yaml', 'r') as file:
    config = yaml.safe_load(file)

# Access the parameters
batch_size = config['batch_size']
learning_rate = config['learning_rate']
epochs = config['epochs']
model_dir = config['model_dir']

# Import the VITS model from the cloned repository
import sys
sys.path.insert(0, './')  # Add VITS to the path
from models import VITS  # Import the actual VITS model

# Initialize the model
model = VITS()  # Make sure to adjust the model initialization if needed

# Set up the optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Define a loss function (adjust based on your use case)
def loss_function(outputs, labels):
    mse_loss = torch.nn.MSELoss()(outputs, labels)
    # Add other losses as necessary
    return mse_loss

# Training loop
for epoch in range(epochs):
    model.train()  # Set the model to training mode
    for mel_spectrogram, text in train_loader:
        optimizer.zero_grad()

        # Forward pass
        mel_spectrogram = mel_spectrogram.unsqueeze(1)  # Adjust shape if necessary
        outputs = model(mel_spectrogram)  # Adjust as per your model's forward function

        # Calculate loss
        labels = ...  # Encode text to numerical labels if necessary
        loss = loss_function(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}')  # Print the loss at the end of each epoch

# Save the model
torch.save(model.state_dict(), 'model_weights.pth')

